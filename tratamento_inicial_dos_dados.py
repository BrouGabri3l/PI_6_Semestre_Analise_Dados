# -*- coding: utf-8 -*-
"""Tratamento_inicial_dos_dados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ehk7QoZiLrYqiNgR4zIK84WEYQuyfqSX
"""

# Montando o drive e realizando as importacoes
import pandas as pd

# Carregando o CSV, tenho uma base original que servira para pegar todos os dados para ser retornado e uma base que sera usada nos treinamentos
original_dataset = pd.read_csv('games_march2025_full.csv')
training_dataset = original_dataset

# Pegando apenas as colunas essenciais
#training_dataset = training_dataset.drop(columns=['price','dlc_count', 'required_age', 'detailed_description', 'about_the_game', 'short_description', 'reviews', 'header_image', 'website', 'support_url', 'support_email', 'metacritic_url', 'metacritic_score', 'achievements', 'notes', 'packages','developers','publishers', 'screenshots', 'movies', 'user_score', 'score_rank', 'estimated_owners', 'average_playtime_forever', 'average_playtime_2weeks', 'median_playtime_forever', 'median_playtime_2weeks', 'discount', 'peak_ccu', 'num_reviews_recent', 'pct_pos_recent'])
training_dataset = training_dataset.drop(columns=['price', 'dlc_count', 'positive', 'negative', 'pct_pos_total','num_reviews_total', 'release_date', 'required_age', 'detailed_description', 'about_the_game', 'reviews', 'website', 'support_url', 'support_email', 'metacritic_url', 'metacritic_score', 'achievements', 'notes', 'packages', 'screenshots', 'developers', 'movies', 'user_score', 'score_rank', 'estimated_owners', 'average_playtime_forever', 'average_playtime_2weeks', 'median_playtime_forever', 'median_playtime_2weeks', 'discount', 'peak_ccu', 'num_reviews_recent', 'pct_pos_recent'])

print(training_dataset)

#Contando duplicados
duplicated = training_dataset.duplicated()
duplicated.sum()

#Removendo duplicados
training_dataset = training_dataset.drop_duplicates()

#Contando duplicados
duplicated = training_dataset.duplicated()
duplicated.sum()

import ast

# Retirando os valores que sao arrays vazios do campo genero
training_dataset['genres'] = training_dataset['genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
training_dataset = training_dataset[training_dataset['genres'].apply(lambda x: isinstance(x, list) and len(x) > 0)]
print(training_dataset)

# Retirando os valores que sao arrays vazios do campo categoria
training_dataset['categories'] = training_dataset['categories'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
training_dataset = training_dataset[training_dataset['categories'].apply(lambda x: isinstance(x, list) and len(x) > 0)]
print(training_dataset)

# Retirando os valores que sao arrays vazios do campo publicantes / desenvolvedoras
training_dataset['publishers'] = training_dataset['publishers'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
training_dataset = training_dataset[training_dataset['publishers'].apply(lambda x: isinstance(x, list) and len(x) > 0)]

print(training_dataset)

training_dataset = training_dataset.dropna(subset=['genres','categories','tags','windows','linux','mac']).reset_index(drop=True)

training_dataset['genres_list']     = training_dataset['genres']
training_dataset['categories_list'] = training_dataset['categories']
training_dataset['publisher_list'] = training_dataset['publishers']
training_dataset['tags_list'] = training_dataset['tags'].apply(lambda t:list(ast.literal_eval(t)))

training_dataset.to_csv("cleaned.csv")